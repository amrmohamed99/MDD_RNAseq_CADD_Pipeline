"""
# Depression Data Analysis and Deep Learning

This notebook combines two potential projects:
1. Analysis of depression-related datasets
2. Deep learning model for image classification (CIFAR-10)

The improved version provides a clear structure and fixes various issues in the original code.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    confusion_matrix, classification_report, accuracy_score,
    roc_curve, auc, precision_recall_curve, f1_score, matthews_corrcoef,
    precision_score, recall_score
)
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization
)
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Set up plotting defaults
plt.style.use('seaborn-whitegrid')
sns.set_palette("muted")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

#-----------------------------------------------------------------------------
# PART 1: DEPRESSION DATA ANALYSIS
#-----------------------------------------------------------------------------

def load_depression_data():
    """
    Load and prepare depression dataset
    
    Returns:
        data_df: Processed dataframe with samples as rows
        labels_df: Dataframe containing labels
    """
    try:
        # Load expression data
        data = pd.read_csv("/kaggle/input/depression/normalized_merged_dataset.csv", index_col=0)
        print(f"Expression data shape: {data.shape}")
        
        # Transpose data so samples are rows
        data_df = data.transpose().reset_index()
        data_df = data_df.rename(columns={'index': 'samples'})
        
        # Load label data
        labels_df = pd.read_excel("/kaggle/input/depression/ML_Conditions DIP.xlsx")
        print(f"Labels data shape: {labels_df.shape}")
        
        return data_df, labels_df
    
    except Exception as e:
        print(f"Error loading depression data: {e}")
        return None, None

def explore_depression_data(data_df, labels_df):
    """
    Perform exploratory data analysis on depression dataset
    
    Args:
        data_df: Expression data dataframe
        labels_df: Labels dataframe
    """
    if data_df is None or labels_df is None:
        print("Data not available for exploration")
        return
    
    print("\n---- Depression Data Summary ----")
    
    # Basic information about the datasets
    print("\nExpression Data:")
    print(f"Number of samples: {data_df.shape[0]}")
    print(f"Number of features: {data_df.shape[1] - 1}")  # Subtract 1 for 'samples' column
    
    print("\nLabels Data:")
    print(f"Number of records: {labels_df.shape[0]}")
    print(f"Columns: {', '.join(labels_df.columns)}")
    
    # Check for missing values
    print("\nMissing values in expression data:", data_df.isnull().sum().sum())
    print("Missing values in labels data:", labels_df.isnull().sum().sum())
    
    # Assuming there's a depression label column, show distribution
    # This is a placeholder - you'll need to modify based on actual data structure
    if 'Depression' in labels_df.columns:
        print("\nDepression label distribution:")
        print(labels_df['Depression'].value_counts())
        
        # Plot distribution
        plt.figure(figsize=(8, 6))
        sns.countplot(x='Depression', data=labels_df)
        plt.title('Depression Status Distribution')
        plt.show()

# Uncomment to execute depression data analysis
# data_df, labels_df = load_depression_data()
# explore_depression_data(data_df, labels_df)

#-----------------------------------------------------------------------------
# PART 2: CIFAR-10 IMAGE CLASSIFICATION
#-----------------------------------------------------------------------------

def load_cifar10_data():
    """
    Load and prepare CIFAR-10 dataset
    
    Returns:
        X_train, y_train: Training data and labels
        X_test, y_test: Testing data and labels
    """
    # Load CIFAR-10 dataset
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # Print dataset information
    print(f"X_train shape: {X_train.shape}")
    print(f"y_train shape: {y_train.shape}")
    print(f"X_test shape: {X_test.shape}")
    print(f"y_test shape: {y_test.shape}")
    
    # Normalize pixel values to be between 0 and 1
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
    
    # One-hot encode labels
    y_train = to_categorical(y_train, 10)
    y_test = to_categorical(y_test, 10)
    
    return X_train, y_train, X_test, y_test

def visualize_cifar10_samples(X_train, y_train):
    """
    Visualize sample images from CIFAR-10 dataset
    
    Args:
        X_train: Training images
        y_train: Training labels (one-hot encoded)
    """
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                  'dog', 'frog', 'horse', 'ship', 'truck']
    
    # Convert one-hot encoded labels back to integers
    y_train_int = np.argmax(y_train, axis=1)
    
    # Plot a grid of images
    plt.figure(figsize=(12, 10))
    for i in range(25):
        plt.subplot(5, 5, i+1)
        plt.imshow(X_train[i])
        plt.title(class_names[y_train_int[i]])
        plt.axis('off')
    plt.tight_layout()
    plt.show()

def create_data_generators(X_train, y_train):
    """
    Create data augmentation generator
    
    Args:
        X_train: Training images
        y_train: Training labels
        
    Returns:
        train_datagen: Configured ImageDataGenerator
    """
    # Data Augmentation
    train_datagen = ImageDataGenerator(
        rotation_range=15,      # Random rotations
        width_shift_range=0.1,  # Random horizontal shifts
        height_shift_range=0.1, # Random vertical shifts
        horizontal_flip=True,   # Random horizontal flips
        zoom_range=0.1,         # Random zoom
        shear_range=0.1,        # Random shear
        fill_mode='nearest'     # Fill strategy for created pixels
    )
    
    # Fit generator on training data
    train_datagen.fit(X_train)
    
    return train_datagen

def create_improved_cnn_model(input_shape=(32, 32, 3), dropout_rate=0.5, l2_reg=0.001):
    """
    Create an improved CNN model for CIFAR-10 classification
    
    Args:
        input_shape: Shape of input images
        dropout_rate: Dropout rate for regularization
        l2_reg: L2 regularization factor
        
    Returns:
        model: Compiled Keras model
    """
    model = Sequential([
        # First Convolutional Block
        Conv2D(64, (3, 3), padding='same', activation='relu', 
               kernel_regularizer=l2(l2_reg), input_shape=input_shape),
        BatchNormalization(),
        Conv2D(64, (3, 3), padding='same', activation='relu', 
               kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(dropout_rate/2),
        
        # Second Convolutional Block
        Conv2D(128, (3, 3), padding='same', activation='relu', 
               kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu', 
               kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(dropout_rate/2),
        
        # Third Convolutional Block
        Conv2D(256, (3, 3), padding='same', activation='relu', 
               kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Conv2D(256, (3, 3), padding='same', activation='relu', 
               kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(dropout_rate/2),
        
        # Flatten and Dense layers
        Flatten(),
        Dense(512, activation='relu', kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(256, activation='relu', kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(10, activation='softmax')  # Output layer for 10 classes
    ])
    
    # Compile model with Adam optimizer and learning rate
    optimizer = Adam(learning_rate=0.001)
    model.compile(
        optimizer=optimizer, 
        loss='categorical_crossentropy', 
        metrics=['accuracy']
    )
    
    # Print model summary
    model.summary()
    
    return model

def train_model(model, train_datagen, X_train, y_train, X_test, y_test, batch_size=64, epochs=50):
    """
    Train the model with callbacks and data augmentation
    
    Args:
        model: Compiled Keras model
        train_datagen: Data augmentation generator
        X_train, y_train: Training data and labels
        X_test, y_test: Testing data and labels
        batch_size: Batch size for training
        epochs: Maximum number of epochs
        
    Returns:
        history: Training history
        model: Trained model
    """
    # Set up callbacks
    callbacks = [
        # Early stopping to prevent overfitting
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        # Model checkpoint to save best model
        ModelCheckpoint(
            'best_cifar10_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),
        # Reduce learning rate when a metric has stopped improving
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        )
    ]
    
    # Train the model with data augmentation
    history = model.fit(
        train_datagen.flow(X_train, y_train, batch_size=batch_size),
        validation_data=(X_test, y_test),
        epochs=epochs,
        steps_per_epoch=X_train.shape[0] // batch_size,
        callbacks=callbacks,
        verbose=2
    )
    
    return history, model

def evaluate_model(model, X_test, y_test):
    """
    Evaluate the model performance on test data
    
    Args:
        model: Trained Keras model
        X_test, y_test: Test data and labels
    """
    # Evaluate on test data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)
    print(f'\nTest Accuracy: {test_acc * 100:.2f}%')
    
    # Generate predictions
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)
    
    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred_classes))
    
    # Confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(12, 10))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues', cbar=False)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()
    
    # Calculate additional metrics
    accuracy = accuracy_score(y_true, y_pred_classes)
    f1 = f1_score(y_true, y_pred_classes, average='weighted')
    mcc = matthews_corrcoef(y_true, y_pred_classes)
    
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score (weighted): {f1:.4f}")
    print(f"Matthews Correlation Coefficient: {mcc:.4f}")
    
    return y_pred, y_true

def plot_training_history(history):
    """
    Plot training history
    
    Args:
        history: Training history from model.fit()
    """
    plt.figure(figsize=(12, 5))
    
    # Plot training & validation accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # Plot training & validation loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

def plot_roc_curves(y_test, y_pred):
    """
    Plot ROC curves for multi-class classification
    
    Args:
        y_test: True labels (one-hot encoded)
        y_pred: Predicted probabilities
    """
    # Binarize the output labels for ROC curve (one-vs-rest approach)
    n_classes = 10
    y_test_binarized = y_test
    y_pred_binarized = y_pred
    
    # Compute ROC curve and ROC AUC for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_binarized[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Compute micro-average ROC curve and ROC AUC
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_binarized.ravel(), y_pred_binarized.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    
    # Plot ROC curves
    plt.figure(figsize=(12, 10))
    
    # Plot micro-average ROC curve
    plt.plot(fpr["micro"], tpr["micro"], color='deeppink', linestyle=':', linewidth=4,
             label=f'Micro-average ROC (AUC = {roc_auc["micro"]:.2f})')
    
    # Plot ROC curves for selected classes to avoid cluttering
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'green', 'red']
    selected_classes = [0, 1, 2, 3, 4]  # Show only 5 classes to keep the plot readable
    
    for i, color in zip(selected_classes, colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                 label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
    
    # Plot diagonal line
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    
    # Set plot properties
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Multi-class Classification')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.show()

#-----------------------------------------------------------------------------
# HYPERPARAMETER TUNING WITH KERAS TUNER
#-----------------------------------------------------------------------------

def create_model_for_tuning(hp):
    """
    Create a model with hyperparameters for tuning
    
    Args:
        hp: Hyperparameter object from Keras Tuner
        
    Returns:
        model: Compiled Keras model with hyperparameters
    """
    # Define hyperparameter search space
    filters_1 = hp.Int('filters_1', min_value=32, max_value=128, step=32)
    filters_2 = hp.Int('filters_2', min_value=64, max_value=256, step=64)
    filters_3 = hp.Int('filters_3', min_value=128, max_value=512, step=128)
    
    dense_units_1 = hp.Int('dense_units_1', min_value=128, max_value=512, step=128)
    dense_units_2 = hp.Int('dense_units_2', min_value=64, max_value=256, step=64)
    
    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)
    learning_rate = hp.Choice('learning_rate', values=[1e-4, 5e-4, 1e-3, 5e-3])
    
    # Build model
    model = Sequential([
        # First Convolutional Block
        Conv2D(filters_1, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(dropout_rate/2),
        
        # Second Convolutional Block
        Conv2D(filters_2, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(dropout_rate/2),
        
        # Third Convolutional Block
        Conv2D(filters_3, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(dropout_rate/2),
        
        # Flatten and Dense layers
        Flatten(),
        Dense(dense_units_1, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(dense_units_2, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate/2),
        Dense(10, activation='softmax')  # Output layer for 10 classes
    ])
    
    # Compile model
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(
        optimizer=optimizer, 
        loss='categorical_crossentropy', 
        metrics=['accuracy']
    )
    
    return model

def run_hyperparameter_tuning(X_train, y_train, X_test, y_test):
    """
    Run hyperparameter tuning using Keras Tuner
    
    Args:
        X_train, y_train: Training data and labels
        X_test, y_test: Test data and labels
        
    Returns:
        best_hps: Best hyperparameters
    """
    import keras_tuner as kt
    
    # Create the tuner
    tuner = kt.RandomSearch(
        create_model_for_tuning,
        objective='val_accuracy',
        max_trials=10,  # Number of different hyperparameter combinations to try
        executions_per_trial=1,  # Number of models to fit per trial
        directory='hyperparameter_tuning',
        project_name='cifar10_tuning',
        overwrite=True
    )
    
    # Set up early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )
    
    # Create data augmentation generator
    datagen = ImageDataGenerator(
        rotation_range=15,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True
    )
    datagen.fit(X_train)
    
    # Run the search
    print("Starting hyperparameter tuning...")
    tuner.search(
        datagen.flow(X_train, y_train, batch_size=64),
        epochs=15,
        validation_data=(X_test, y_test),
        steps_per_epoch=X_train.shape[0] // 64,
        callbacks=[early_stopping],
        verbose=1
    )
    
    # Get the best hyperparameters
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    
    print("Best Hyperparameters:")
    print(f"- filters_1: {best_hps.get('filters_1')}")
    print(f"- filters_2: {best_hps.get('filters_2')}")
    print(f"- filters_3: {best_hps.get('filters_3')}")
    print(f"- dense_units_1: {best_hps.get('dense_units_1')}")
    print(f"- dense_units_2: {best_hps.get('dense_units_2')}")
    print(f"- dropout_rate: {best_hps.get('dropout_rate')}")
    print(f"- learning_rate: {best_hps.get('learning_rate')}")
    
    return best_hps

def train_best_model(best_hps, X_train, y_train, X_test, y_test):
    """
    Train the model with the best hyperparameters
    
    Args:
        best_hps: Best hyperparameters from tuning
        X_train, y_train: Training data and labels
        X_test, y_test: Test data and labels
        
    Returns:
        model: Best trained model
    """
    # Build the model with the best hyperparameters
    best_model = create_model_for_tuning(best_hps)
    
    # Set up callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),
        ModelCheckpoint('best_tuned_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)
    ]
    
    # Create data augmentation generator
    datagen = ImageDataGenerator(
        rotation_range=15,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True,
        zoom_range=0.1
    )
    datagen.fit(X_train)
    
    # Train the model
    print("\nTraining model with best hyperparameters...")
    history = best_model.fit(
        datagen.flow(X_train, y_train, batch_size=64),
        validation_data=(X_test, y_test),
        epochs=50,
        steps_per_epoch=X_train.shape[0] // 64,
        callbacks=callbacks,
        verbose=2
    )
    
    # Plot training history
    plot_training_history(history)
    
    # Evaluate the model
    print("\nEvaluating tuned model:")
    evaluate_model(best_model, X_test, y_test)
    
    return best_model

#-----------------------------------------------------------------------------
# MAIN EXECUTION
#-----------------------------------------------------------------------------

def main():
    """
    Main execution function
    """
    print("===== CIFAR-10 Image Classification with Deep Learning =====")
    
    # Load CIFAR-10 data
    X_train, y_train, X_test, y_test = load_cifar10_data()
    
    # Visualize samples
    visualize_cifar10_samples(X_train, y_train)
    
    # Create data generators
    train_datagen = create_data_generators(X_train, y_train)
    
    # Create model
    model = create_improved_cnn_model()
    
    # Train model
    history, trained_model = train_model(model, train_datagen, X_train, y_train, X_test, y_test)
    
    # Plot training history
    plot_training_history(history)
    
    # Evaluate model
    y_pred, y_true = evaluate_model(trained_model, X_test, y_test)
    
    # Plot ROC curves
    plot_roc_curves(y_test, y_pred)
    
    # Hyperparameter tuning (optional - uncomment to run)
    # best_hps = run_hyperparameter_tuning(X_train, y_train, X_test, y_test)
    # best_model = train_best_model(best_hps, X_train, y_train, X_test, y_test)
    
    print("===== Analysis Complete =====")

# Execute main function
if __name__ == "__main__":
    main()
